# ğŸ“˜ Colab Notebook: GPT-2 small ã‚’é’ç©ºæ–‡åº«ã§å­¦ã¶ã€Œåˆå¿ƒè€…å‘ã‘æ•™æã€ç‰ˆï¼ˆä¿®æ­£ç‰ˆï¼‰
# ã‚¹ãƒãƒ›ã§ã‚‚ã‚³ãƒ”ãƒš1å›ã§å‹•ã‹ã›ã‚‹ã‚ˆã†ã«ã€å…¨éƒ¨ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚

# ==============================
# ã‚»ãƒ« 1: å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# ==============================

# ==============================
# ã‚»ãƒ« 2: GPU ç¢ºèª
# ==============================
import torch
print('Torch ãƒãƒ¼ã‚¸ãƒ§ãƒ³:', torch.__version__)
print('GPUãŒä½¿ãˆã‚‹ã‹:', torch.cuda.is_available())
if torch.cuda.is_available():
    print('GPUã®åå‰:', torch.cuda.get_device_name(0))

# ==============================
# ã‚»ãƒ« 3: é’ç©ºæ–‡åº«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
# ==============================
import requests, re
url = "https://www.aozora.gr.jp/cards/001235/files/49866_41897.html"
resp = requests.get(url)
resp.encoding = resp.apparent_encoding
html = resp.text
text = re.sub(r'<[^>]+>', '', html)
text = re.sub(r"\r", "", text)
text = re.sub(r"\n{2,}", "\n", text)
open('data/data_text_aozora.txt', 'w', encoding='utf-8').write(text)
print('ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆä¿å­˜ã€‚æ–‡å­—æ•°=', len(text))

# ==============================
# ã‚»ãƒ« 4: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æº–å‚™
# ==============================
from transformers import GPT2TokenizerFast
tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')
print('èªå½™ã‚µã‚¤ã‚º:', tokenizer.vocab_size)

# ==============================
# ã‚»ãƒ« 5: ãƒ‡ãƒ¼ã‚¿ã‚’æ•°å­—åŒ–
# ==============================
from datasets import Dataset
with open('data/data_text_aozora.txt', 'r', encoding='utf-8') as f:
    raw = f.read()

encodings = tokenizer(raw, return_tensors='pt', add_special_tokens=False, truncation=True, max_length=80000)
input_ids = encodings['input_ids'][0]

block_size = 128
examples = []
for i in range(0, input_ids.size(0) - block_size + 1, block_size):
    block = input_ids[i:i+block_size]
    examples.append({'input_ids': block.tolist(), 'attention_mask': [1]*block_size})

print('ã‹ãŸã¾ã‚Šæ•°:', len(examples))
dataset = Dataset.from_list(examples)

# ==============================
# ã‚»ãƒ« 6: ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
# ==============================
from transformers import GPT2LMHeadModel
model = GPT2LMHeadModel.from_pretrained('gpt2')
print('ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ')

# ==============================
# ã‚»ãƒ« 7: å­¦ç¿’è¨­å®š
# ==============================
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./gpt2_wikipedia',
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=1,     # ã¾ãšã¯1ã‚¨ãƒãƒƒã‚¯
    learning_rate=5e-5,
    fp16=torch.cuda.is_available(),
    logging_steps=50,
    save_steps=200,
    save_total_limit=2,
    report_to="none",   # â† wandb ã‚’ç„¡åŠ¹åŒ–
)

def data_collator(features):
    import torch
    input_ids = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)
    attention_mask = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)
    labels = input_ids.clone()
    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
)

# ==============================
# ã‚»ãƒ« 8: å­¦ç¿’å®Ÿè¡Œ
# ==============================
print("å­¦ç¿’ã‚’å§‹ã‚ã¾ã™...")
trainer.train()

# ==============================
# ã‚»ãƒ« 9: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
# ==============================
prompt = "ç§ã¯"
inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
input_ids = inputs["input_ids"]
attention_mask = inputs["attention_mask"]
out = model.generate(input_ids, max_length=80, do_sample=True, top_k=50, top_p=0.95, attention_mask=attention_mask, pad_token_id=tokenizer.pad_token_id)
print("ç”Ÿæˆã•ã‚ŒãŸæ–‡ç« :")
print(tokenizer.decode(out[0], skip_special_tokens=True))

# ==============================
# ä¿å­˜
# ==============================
#save_path = "gpt2_aozora/trained"
#model.save_pretrained(save_path)
#tokenizer.save_pretrained(save_path)
trainer.push_to_hub("first-pretrain")
tokenizer.push_to_hub("first-pretrain")

# ==============================
# âœ… ã¾ã¨ã‚
# - LossãŒä¸‹ãŒã‚‹ã“ã¨ã‚’è¦‹ã¦å­¦ç¿’ãŒé€²ã‚€å®Ÿæ„Ÿã‚’å¾—ã‚‰ã‚Œã‚‹
# - å­¦ç¿’å¾Œã«æ—¥æœ¬èªã£ã½ã„æ–‡ç« ãŒå‡ºã¦ãã‚‹
# - å°ã•ãªã€Œäº‹å‰å­¦ç¿’ä½“é¨“ã€ã‚’ã‚¹ãƒãƒ›ã‹ã‚‰ã§ã‚‚å®Ÿè¡Œã§ãã‚‹ï¼
